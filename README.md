# Επεξεργασία Φυσικής Γλώσσας

## Παραδοτέο 1Α:
**Σκοπός:** να κάνουμε αναδιατύπωση και βασική δίορθωση γραμματικής σε αγγλικές προτάσεις, με χρήση NLP εραγλείων και τεχνικών

**Επεξήγηση:**

Αρχικά, φροτώνουμε τις βασικές βιβλιοθήκες που θα χρησιμοποιήσουμε στο κομμάτι του κώδικα. Τους transformers του χρησιμοποιούμε προκειμενου να φορτώσουμε το T5 μοντέλο παραφράσεων από το HuggingFace. Το spacy και nltk μας βοηθούν για tokenization και part of speech tagging.

![code](/images/Screenshot%202025-06-20%20195818.png)

-->getTags(text): Χρησιμοποιεί το Spacy προκειμένου να πάρουμε τη λίστα με τη μορφή (λέξη, γραμματικο tag) για κάθε λέξη του κειμένου που περνά ως όρισμα.

![code](/images/Screenshot%2025-06-20%200012.png)

-->correct_grammar(text): Στη συγκεκριμένη συνάρτηση παίρνουμε τη λίστα από το getTags και για κάθε λέξη παίρνουμε access στην ίδια τη λέξη αλλά και το tag τους για να βρούμε τι μέρος του λόγου είναι. Από εκεί εφαρμόζουμε απλούς χειροποίητους κανόνες, οι οποίοι είναι:
+Αν η πρόταση ξεκινάει με ουσιαστικό χώρις άρθρο, προσθέτουμε το 'a'.
+Αν δύο ουσιαστικά εμφανίζονται διαδοχικά, τότε προσθέτουμε ανάμεσά τους ένα of.
+Αν βρεθεί η λέξη at πριν από χρονική λέξη, αντικαθιστάται με το in.

![code](/images/Screenshot%2025-06-20%200132.png)

-->paraphrase(text): Εδώ εφαρμόζεται το T5 μοντέλο για να αναδιατυπώσουμε το κείμενο του ορίσματος. Χρησιμοποιούμε ρητή μορφή για paraphrase. Στο τέλος, παράγουμε ένα κείμενο με τις εξής παραμέτρους:
+no_repeat_ngram_size=2, αποτρέπει την επανάληψη διπλών λέξεων.
+num_beams=5, Beam search με 5 διαδρομές για ποιοτική έξοδο.
+early_stopping=True, σταματά την αναζήτηση όταν βρεθεί επαρκής πρόταση.

![code](/images/Screenshot%2025-06-20%200156.png)

Η έξοδος του μοντέλου μετατρέπεται ξανά σε κανονικό string αφαιρώντας tokens όπως </s>,<pad>.

## Παραδοτέο 1B:
**Ο κώδικας στο συγκεκριμένο ερώτημα:**Αξιοποιεί το pipeline API από τη βιβλιοθήκη transformers της Hugging Face, για να μπορέσουμε να υλοποιήουμε αυτόματη παράφραση αγγλικών προτάσεων.

**Δομή:**Οι κώδικες περιλάμβάνουν (1) τη φόρτωση κατάλληλου μοντέλου μέσω pipeline,(2) ορισμό κειμένου για παράφραση, (3) εκτέλεση της παράφρασης μέσω της συνάρτησης pipeline με ενεργή ρύθμιση για ποικιλία και (4) εκτύπωση του αποτελέσματος.

Μοντέλο Bart:
![code](/images/Screenshot%2025-06-20%203130.png)
Μοντέλο T5:
![code](/images/Screenshot%2025-06-20%203210.png)
Μοντέλο Pegasus:
![code](/images/Screenshot%2025-06-20%203225.png)

Και τα τρία έχουν τις παραμέτρους:
+max_length=256, μέγιστο μήκος παραγόμενου κειμένου
+do_sample=True, ενεργοποιεί τυχαιότητα για πιο ποικιλόμορφη και δημιουργική έξοδο.

## Παραδοτέο 1C:
**Σκοπός:**για να εκτιμήσουμε την ποιότητα των παραφράσεων που παράγουν τα μοντέλα Bart, T5, Pegasus, χρησιμοποιούμε το **metric BertScore**, το οποίο αξιολογεί τη σημασιολογική ομοιόητα μτεαξύ της αρχικής και της παραφρασμένης πρότασης.

**BERTScore:** αξιολογεί ένα παραγομένο κείμενο με βάση τις εννοιολογικές σχέσεις που προκύπτουν από τα embedding του προεκπαιδευμένου μοντέλου BERT. Για λόγους ταχύτητας χρησιμοποιούμε το μοντέλο bert-base-uncased και επιστρέφει:
+P,ποσό καλά τα παραγόμενα tokens ταιριάζουν με το αναμενόμενο.
+R, πόσο καλά το αναμενόμενο καλύπτει το παραγόμενο.
+F ο μέσος όρος αυτών των δύο. Αυτό διατήρουμε στον συγκεντροτικό πίνακα αποτελεσμάτων που δημιουργούμε με τη βοήθεια του pandas.

![code](/images/Screenshot%2025-06-20%210029.png)

## Παραδοτεό 2:

**Σκοπός:**το παραδοτέο στοχεύει να μετρηθεί η σημασιολογική ομοιότητα μεταξύ αρχικών και παραφρασμένων προτάσεων και να οποτικοποηθεί η εγγύτητα με τη χρήση του PCA.

Με τη βοήθεια του μοντέλου all-MiniLM-L6-v2 που παρέχει η Sentence Transformes, μετατρέπουμε τα κείμενα σε πυκνά διανύσματα.

### Μέρος 1Α
Το συγκεκριμένο μέρος αφορά την ομοιότητα προτάσεων με Cosine Similarity, για κάθε πρόταση παράγονται τα διανύσματα embedding μέσω του SentenceTransformer και υπολογίζεται το συνημίτονο της γωνίας. Με λίγα λόγια, πόσο κόντα βρίσκονται στο νοηματικό χώρο. Χρησιμοποιούμε για ευκολότερη συγκριτική ερμηνεία μόνο το πρώτο δεκαδικό και έχει διάστημα τιμών με [0,1], με 1 να είναι η πλήρης ταύτιση.

![code](/images/Screenshot%2025-06-20%212505.png)

### Μέρος 1Β
Το μόνο διαφορετικό κομμάτι είναι ότι δεν εξετάζουμε μεμονωμένες προτάσεις, εξετάζονται ολόκληρα τα κείμενα,δηλαδή το αρχικό μαζί με τις παραφράσεις που δημιουργήθηκαν από το BART, T5 ,Pegasus.

![code](/images/Screenshot%2025-06-20%212426.png)

Ο υψηλότερος βαθμός υποδηλώνει καλύτερη διατήρηση του αρχικου νοήματος.

### Οπτικοποιήση PCA

Κάθε σύνολο κειμένων μετατρέπεται σε 384-διάστατα διανύσματα. Αυτό το κάνουμε προκειμένου να οπτικοποιήσουμε τις θέσεις των αρχικών και παραφρασμένων κειμένων στον χώρο, εμφαρμόζοντας PCA σε 2 διαστάσεις. Το κάθε διάγραμμα δείχνει πόσο κόντα βρίσκονται οι παραφράσεις από το κείμενο.

![code](/images/Screenshot%2025-06-20%212350.png)

Οι τελικες 2D συντεταγμένες απεικονίζονται με matplotlib σε διάγραμμα διασποράς.

**Κοντινα σημεία:**τα αντίστοιχα κείμενα έχουν παρόμοιο νοήμα

**Εφαρμογή σε δύο σενάρια:** Η παραπάνω λογική εφαρμόζεται και για τα δύο κείμενα που μας δίνονται στην εκφώνηση της άσκησης.

